# PageCrunch

**ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ç‰¹å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é«˜é€Ÿã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€URL é‡è¤‡æ’é™¤æ©Ÿèƒ½ä»˜ãã§ AI ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«æœ€é©ãª JSONL å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼**

---

## âœ¨ ç‰¹é•·

| æ©Ÿèƒ½                | èª¬æ˜                                                       |
| ------------------- | ---------------------------------------------------------- |
| ã‚µãƒ–ãƒ„ãƒªãƒ¼é™å®šã‚¯ãƒ­ãƒ¼ãƒ« | ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ãƒ‘ã‚¹ãƒ—ãƒ¬ãƒ•ã‚£ã‚¯ã‚¹ï¼ˆä¾‹: `/blog/`ï¼‰ã®ä¸¡æ–¹ã§åˆ¶é™ã—ã€å¿…è¦ãªãƒšãƒ¼ã‚¸ã ã‘å–å¾—ã€‚ |
| JSON Lines å‡ºåŠ›     | 1è¡Œ1ãƒšãƒ¼ã‚¸å½¢å¼ã§ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã€åŸ‹ã‚è¾¼ã¿ã€å¤§è¦æ¨¡RAGã«æœ€é©ã€‚      |
| å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè¡Œ      | `scrapy runspider page_crunch.py` ã ã‘ã§å®Ÿè¡Œå¯èƒ½ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆä¸è¦ã€‚ |
| Robots å¯¾å¿œ & ä¸å¯§ãªã‚¯ãƒ­ãƒ¼ãƒ« | `robots.txt` ã‚’éµå®ˆã—ã€å¾…æ©Ÿæ™‚é–“ã‚„ä¸¦åˆ—æ•°ã‚’èª¿æ•´å¯èƒ½ã€‚          |
| URL é‡è¤‡æ’é™¤         | SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã‚‹ URL ãŠã‚ˆã³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡è¤‡å›é¿ã€‚AI ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«æœ€é©ã€‚ |
| ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥è¿½è·¡  | SHA-256 ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ç•°ãªã‚‹ URL ã®é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œå‡ºã€‚   |
| æ°¸ç¶šçš„è¿½è·¡           | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯å®Ÿè¡Œé–“ã§ä¿æŒã•ã‚Œã€éå»ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãŸ URL ã‚’å†è¨ªå•ã—ãªã„ã€‚ |
| ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºç°¡å˜      | å¼•æ•°ã‚„ Spider å±æ€§ã§ã»ã¼ã™ã¹ã¦è¨­å®šå¯èƒ½ã€‚                      |

---

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å‰ææ¡ä»¶

* Python 3.9 ä»¥ä¸Š
* pip / venv (`sudo apt install python3-pip python3-venv` on Debian/Ubuntu/WSL)
* ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µã‚¤ãƒˆã¸ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç’°å¢ƒ

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ã‚³ãƒ”ãƒ¼
mkdir pagecrunch && cd pagecrunch
cp /path/to/page_crunch.py .

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆå¼·ãæ¨å¥¨ï¼‰
python3 -m venv .venv
source .venv/bin/activate

# Scrapy ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip scrapy
```

---

## ğŸš€ ä½¿ã„æ–¹

```bash
# åŸºæœ¬: https://example.com/sometips/ é…ä¸‹ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ corpus.jsonl ã«å‡ºåŠ›
scrapy runspider page_crunch.py \
  -a start_url=https://example.com/sometips/ \
  -a domain=example.com \
  -a db_path=example_urls.db \
  -o corpus.jsonl
```

### å‡ºåŠ›å½¢å¼ (JSONL)

```jsonc
{"url":"https://example.com/sometips/foo","title":"Foo Tips","content":"ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ†ã‚­ã‚¹ãƒˆ...","content_hash":"a1b2c3...","crawled_at":"2025-05-07T06:45:12.345678","status":200,"length":12345}
{"url":"https://example.com/sometips/bar","title":"Bar Tricks","content":"ã•ã‚‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ†ã‚­ã‚¹ãƒˆ...","content_hash":"d4e5f6...","crawled_at":"2025-05-07T06:45:15.678901","status":200,"length":6789}
```

å„è¡Œã¯ç‹¬ç«‹ã—ãŸ JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€ãƒ™ã‚¯ã‚¿ãƒ¼DB ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚„è¿½åŠ ã® Markdown å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®ãƒ‘ã‚¤ãƒ—ã«æœ€é©ã§ã™ã€‚

### URL è¿½è·¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹

PageCrunch ã¯ URL è¿½è·¡ç”¨ã® SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆãƒ»ç®¡ç†ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

* è¨ªå•æ¸ˆã¿ URL ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨å…±ã«ä¿å­˜
* å¾Œç¶šã®å®Ÿè¡Œã§åŒã˜ URL ã‚’å†è¨ªå•ã—ãªã„ã‚ˆã†é˜²æ­¢
* ç•°ãªã‚‹ URL ã§ã®é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œå‡º
* Write-Ahead Logging (WAL) ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Š

ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ç›´æ¥ã‚¯ã‚¨ãƒªå¯èƒ½ã§ã™ï¼š

```bash
sqlite3 example_urls.db "SELECT url, visited_at FROM visited_urls ORDER BY visited_at DESC LIMIT 10;"
```

---

## âš™ï¸ Spider å¼•æ•°

| å¼•æ•°             | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ           | èª¬æ˜                                           |
| ---------------- | ------------------- | ---------------------------------------------- |
| `start_url`      | (å¿…é ˆ)               | ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ URLï¼ˆã‚µãƒ–ãƒ„ãƒªãƒ¼ã®ãƒ«ãƒ¼ãƒˆï¼‰ã€‚        |
| `domain`         | (å¿…é ˆ)               | ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¶é™ï¼ˆä¾‹: `example.com`ï¼‰ã€‚            |
| `path_prefix`    | `start_url` ã¨åŒã˜   | ã“ã®æ–‡å­—åˆ—ã§å§‹ã¾ã‚‹ URL ã®ã¿è¿½è·¡ã€‚              |
| `db_path`        | `pagecrunch_urls.db` | URL è¿½è·¡ç”¨SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹ã€‚`:memory:` ã§ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã€‚ |
| `download_delay` | `0.5`               | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿç§’æ•° (`-s DOWNLOAD_DELAY=0.1` ã§å¤‰æ›´å¯)ã€‚ |
| `concurrency`    | `8`                 | åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° (`-s CONCURRENT_REQUESTS=16` ã§å¤‰æ›´å¯)ã€‚ |

---

## ğŸ” é«˜åº¦ãªä½¿ç”¨ä¾‹

### ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª URL è¿½è·¡

æ°¸ç¶šçš„ãª URL è¿½è·¡ãŒä¸è¦ãªä¸€æ™‚çš„ãªã‚¯ãƒ­ãƒ¼ãƒ«ã®å ´åˆï¼š

```bash
scrapy runspider page_crunch.py \
  -a start_url=https://docs.example.com/api/ \
  -a domain=docs.example.com \
  -a db_path=:memory: \
  -o api_docs.jsonl
```

### ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¤‡åˆ†æ

ã‚µã‚¤ãƒˆä¸Šã®é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é‡ã‚’åˆ†æã™ã‚‹ã«ã¯ï¼š

```bash
sqlite3 example_urls.db "SELECT content_hash, COUNT(*) as num_duplicates FROM visited_urls GROUP BY content_hash HAVING COUNT(*) > 1 ORDER BY num_duplicates DESC;"
```

---

## ğŸ›  é–‹ç™ºãƒ¡ãƒ¢

* **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** â€“ WSL ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€Windows ãƒ‰ãƒ©ã‚¤ãƒ–ï¼ˆ`/mnt/c/...`ï¼‰ã§ã¯ãªã Linux ãƒ›ãƒ¼ãƒ ï¼ˆ`~/projects/pagecrunch`ï¼‰ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ I/O ãŒé«˜é€ŸåŒ–ã—ã¾ã™ã€‚
* **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹** â€“ å¤§è¦æ¨¡ã‚¯ãƒ­ãƒ¼ãƒ«ã§ã¯ã€å®šæœŸçš„ãªãƒã‚­ãƒ¥ãƒ¼ãƒ å‡¦ç†ã‚’æ¤œè¨ï¼š`sqlite3 example_urls.db "VACUUM;"`
* **æ‹¡å¼µ** â€“ `yield` éƒ¨åˆ†ã‚’å¤‰æ›´ã—ã¦ã€å¯èª­æ€§/Markdown æŠ½å‡ºã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¼·åŒ–ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

---

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆ

ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­“è¿ã§ã™ï¼å¤§ããªå¤‰æ›´ã‚’åŠ ãˆã‚‹å‰ã«ã€ã¾ãšã‚¤ã‚·ãƒ¥ãƒ¼ã‚’é–‹ã„ã¦è­°è«–ã—ã¦ãã ã•ã„ã€‚

1. Fork â†’ ãƒ–ãƒ©ãƒ³ãƒä½œæˆ â†’ PR
2. å¯èƒ½ãªå ´åˆã¯ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¿½åŠ 
3. `flake8` / `black` ã®ãƒ‘ã‚¹ã‚’ç¢ºèª

---

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

```
Copyright 2025 Qadiff LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

---

## ğŸ“« ãŠå•ã„åˆã‚ã›

* **ä¼šç¤¾å**: Qadiff LLC
* **Web**: [https://qadiff.com](https://qadiff.com)
* **Twitter**: @Qadiff

Happy crawling! ğŸ•·ï¸